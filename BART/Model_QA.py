# -*- coding: utf-8 -*-
"""MODEL_QA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jDiZbKqBcMUBgkpOvbjKQGfnFDRGi5fS
"""
from pinecone import Pinecone
import time
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM



def query_pinecone(query, top_k):
    # Generate embeddings for the query
    xq = retriever.encode([query]).tolist()

    # Search the first Pinecone index (index_1) for the answer
    xc_qa = index_1.query(vector=xq, top_k=top_k, include_metadata=True)

    # Search the second Pinecone index (index_2) for context passage
    xc_chunk = index_2.query(vector=xq, top_k=top_k, include_metadata=True)

    return xc_qa, xc_chunk



def generate_model_input(query_and_docs):
    """Tokenize the input for the BART model."""
    return tokenizer(
        query_and_docs,
        truncation=True,
        padding="max_length",
        return_tensors="pt",
        max_length=512  # Adjust based on input size
    )

def generate_model_answer(retrieved_content,query):
    """Generate the answer using the BART model."""
    
    query_and_docs = query + " " + retrieved_content  # Combine query and context (retrieved content)
    
    # Tokenize the combined input (query + retrieved content)
    model_input = generate_model_input(query_and_docs)
    
    generated_answers_encoded = model.generate(
        input_ids=model_input["input_ids"].to(device),
        attention_mask=model_input["attention_mask"].to(device),
        min_length=30,
        max_length=512,
        num_beams=5,  # Explore multiple sequences
        early_stopping=True,  # Stop when the best sequence is found
        eos_token_id=tokenizer.eos_token_id,
        no_repeat_ngram_size=2
    )

    # Decode the generated answer
    decoded_answers = tokenizer.batch_decode(
        generated_answers_encoded,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=True
    )

    return decoded_answers



    
def generate_answer(query1):
    """Main function to process the query and generate an answer."""
    # Step 1: Obtain responses from both indices (Pinecone queries)
    qa_result, chunk_result = query_pinecone(query1, top_k=2)

    # Step 2: Filter out results with score below 0.3 and prepare the context
    qa_context = " ".join([match['metadata']['Generated_Answer'] for match in qa_result['matches'] if match['score'] >= 0.3])
    chunk_context = " ".join([match['metadata']['Chunk'] for match in chunk_result['matches'] if match['score'] >= 0.3])

    # If no content was retrieved with a score >= 0.3, return "Answer not found"
    if not qa_context and not chunk_context:
        return ["Answer not found"]

    # Combine the retrieved contexts into one content string
    retrieved_content = f"{qa_context} {chunk_context}"

    # Step 3: Generate the answer using the combined retrieved content
    decoded_answers = generate_model_answer(retrieved_content, query1)

    return decoded_answers
# Set the API key directly in the code
api_key = 'df47aa2a-f42c-4446-85bc-e2a4af03f79d'

# Initialize the Pinecone client using the API key
pc = Pinecone(api_key=api_key)

device = 'cuda' if torch.cuda.is_available() else 'cpu'

index_name_1 = "hydro-question-answer"
index_name_2 = "hydro-chunk"

# Connect to the question index
index_1 = pc.Index(index_name_1)
# Connect to the question index
index_2 = pc.Index(index_name_2)

retriever = SentenceTransformer("flax-sentence-embeddings/all_datasets_v3_mpnet-base", device=device)


model_name = "1MK26/Final_FT_BART"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
model = model.to(device)

