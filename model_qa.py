# -*- coding: utf-8 -*-
"""MODEL_QA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jDiZbKqBcMUBgkpOvbjKQGfnFDRGi5fS
"""

pip install pinecone sentence-transformers transformers

from pinecone import Pinecone
import time
import torch
from sentence_transformers import SentenceTransformer
from transformers import BartTokenizer, BartForConditionalGeneration

def generate_answer(query1):
    """Main function to process the query and generate an answer."""
    # Step 1: Obtain responses from both indices
    qa_result, chunk_result = query_pinecone(query1, top_k=2)

    # Step 2: Format the query with the context
    query_and_docs = format_query(query1, qa_result['matches'], chunk_result['matches'])

    # Step 3: Tokenize the input for the BART model
    model_input = generate_model_input(query_and_docs)

    # Step 4: Generate the answer using the model
    decoded_answers = generate_model_answer(model_input)

    return decoded_answers

def query_pinecone(query, top_k):
    # Generate embeddings for the query
    xq = retriever.encode([query]).tolist()

    # Search the first Pinecone index (index_1) for the answer
    xc_qa = index_1.query(vector=xq, top_k=top_k, include_metadata=True)

    # Search the second Pinecone index (index_2) for context passage
    xc_chunk = index_2.query(vector=xq, top_k=top_k, include_metadata=True)

    return xc_qa, xc_chunk

def format_query(query, qa_context, chunk_context):
    """Format the query with the combined context."""
    # Extract Generated Answer from the QA context and add the <P> tag
    qa_context = [f"<P> {m['metadata']['Generated_Answer']}" for m in qa_context]

    # Extract Chunk from the chunk context and add the <P> tag
    chunk_context = [f"<P> {m['metadata']['Chunk']}" for m in chunk_context]

    # Join the contexts into a single string
    combined_context = " ".join(qa_context + chunk_context)

    # Format the query with the question and the combined context
    formatted_query = f"question: {query} context: {combined_context}"
    return formatted_query

def generate_model_input(query_and_docs):
    """Tokenize the input for the BART model."""
    return tokenizer(
        query_and_docs,
        truncation=True,
        padding="max_length",
        return_tensors="pt",
        max_length=256  # Adjust based on input size
    )

def generate_model_answer(model_input):
    """Generate the answer using the BART model."""
    generated_answers_encoded = model.generate(
        input_ids=model_input["input_ids"].to(device),
        attention_mask=model_input["attention_mask"].to(device),
        min_length=30,  # Ensuring that the answer isn't too short
        max_length=256,  # Limiting the length to avoid overly long or irrelevant responses
        do_sample=True,  # Enable sampling for diversity
        top_p=0.85,  # Using nucleus sampling for more diverse but focused output
        temperature=0.7,  # Lower temperature for more deterministic output
        eos_token_id=tokenizer.eos_token_id,
        no_repeat_ngram_size=2,  # Avoid repeating n-grams
        num_return_sequences=1  # Generate only the best sequence
    )

    # Decode the generated answer
    decoded_answers = tokenizer.batch_decode(
        generated_answers_encoded,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=True
    )

    return decoded_answers

# Set the API key directly in the code
api_key = 'df47aa2a-f42c-4446-85bc-e2a4af03f79d'

# Initialize the Pinecone client using the API key
pc = Pinecone(api_key=api_key)

device = 'cuda' if torch.cuda.is_available() else 'cpu'

index_name_1 = "hydro-question-answer"
index_name_2 = "hydro-chunk"

# Connect to the question index
index_1 = pc.Index(index_name_1)
# Connect to the question index
index_2 = pc.Index(index_name_2)

retriever = SentenceTransformer("flax-sentence-embeddings/all_datasets_v3_mpnet-base", device=device)

tokenizer = BartTokenizer.from_pretrained('vblagoje/bart_lfqa')
generator = BartForConditionalGeneration.from_pretrained('vblagoje/bart_lfqa').to(device)

